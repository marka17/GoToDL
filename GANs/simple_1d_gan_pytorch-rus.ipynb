{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> ( __@this notebook__ will guide you through a very simple case of generative adversarial networks.) \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/model.png\" width=320px height=240px>\n",
    "\n",
    "\n",
    "[//]: <> ( Like.. veeery simple. Generative adversarial networks that learn to convert 1d uniform noise distribution to a uniform 1d normal data distribution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В этом ноутбуке мы попробуем генерировать одномерное нормальное распределение из равномерного распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут генератор пытается преобразовать шум в реальные данные\n",
    "gen = nn.Sequential(nn.Linear(1, 16), nn.ELU(), nn.Linear(16, 1))\n",
    "gen_opt = torch.optim.SGD(gen.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# А дискриминатор пытается отличить сгенерированные данные от настоящих\n",
    "# Он возвращает массив из двух чисел\n",
    "# Где первое число равное вероятности пренадлежности объекта к сгенерированным данным \n",
    "# А второе к реальным данным\n",
    "disc = nn.Sequential(nn.Linear(1, 64), nn.ELU(), nn.Linear(64, 2))\n",
    "disc_opt = torch.optim.SGD(disc.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_FAKE, IS_REAL = 0, 1\n",
    "\n",
    "def train_disc(batch_size):\n",
    "    \"\"\"\n",
    "    Один шаг дискриминатора\n",
    "    \"\"\"\n",
    "    \n",
    "    # Посчитаем log p(real | x) \n",
    "    real_data = sample_real_data(batch_size)\n",
    "    logp_real_is_real = F.log_softmax(disc(real_data), dim=1)[:, IS_REAL]\n",
    "    \n",
    "    # Посчитаем logp(fake | G(z))\n",
    "    noise = <sample noise>\n",
    "    gen_data = <generate data given noise>\n",
    "    logp_gen_is_fake = <compute logp for 0th>\n",
    "    \n",
    "    disc_loss = <compute loss>\n",
    "    \n",
    "    # Градиентный шаг\n",
    "    disc_opt.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    disc_opt.step()\n",
    "    return disc_loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(batch_size):\n",
    "    \"\"\" \n",
    "    Один шаг генератора\n",
    "    \"\"\"\n",
    "        \n",
    "    # Посчитаем log p(fake | G(z)).\n",
    "    noise = <sample noise>\n",
    "    gen_data = <generate data given noise>\n",
    "    \n",
    "    logp_gen_is_real = <compute logp gen is REAL>\n",
    "    \n",
    "    gen_loss = <generator loss>\n",
    "    \n",
    "    gen_opt.zero_grad()\n",
    "    gen_loss.backward()\n",
    "    gen_opt.step()\n",
    "    return gen_loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "for i in range(100000):\n",
    "\n",
    "    for _ in range(5):\n",
    "        train_disc(128)\n",
    "    \n",
    "    train_gen(128)\n",
    "    \n",
    "    if i % 250 == 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[14, 6])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Data distributions\")\n",
    "        plt.hist(gen(sample_noise(1000)).data.numpy()[:, 0], range=[0, 10], alpha=0.5, normed=True, label='gen')\n",
    "        plt.hist(sample_real_data(1000).data.numpy()[:,0], range=[0, 10], alpha=0.5, normed=True, label='real')\n",
    "        \n",
    "        x = np.linspace(0,10, dtype='float32')\n",
    "        disc_preal = F.softmax(disc(Variable(torch.from_numpy(x[:, None]))))[:, 1]\n",
    "        plt.plot(x, disc_preal.data.numpy(), label='disc P(real)')\n",
    "        plt.legend()\n",
    "        \n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Discriminator readout on real vs gen\")\n",
    "        plt.hist(F.softmax(disc(gen(sample_noise(100))))[:, 1].data.numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(G(z))')\n",
    "        plt.hist(F.softmax(disc(sample_real_data(100)))[:, 1].data.numpy(),\n",
    "                 range=[0, 1], alpha=0.5, label='D(x)')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Что ожидается увидеть на графиках:__\n",
    "* __Левый:__ Два распределения начнутся по-разному, но распределение генератора должно соответствовать реальным данным почти везде. Кривая представляет мнение дискриминатора по всем возможным значениям x. Он должен медленно приближаться к 0,5 над областями, где реальные данные плотны.\n",
    "* __Правый:__ На этой диаграмме показано, как часто дискриминатор назначает заданную вероятность выборкам из реальных и сгенерированных выборок данных (показано разными цветами). Первые несколько итераций будут отличаться, но в конечном итоге они оба будут иметь почти всю массу вероятности около 0,5, поскольку генератор становится лучше в своей работе.\n",
    " * Если вместо этого она сходится вокруг 0 (gen) и 1(real), Ваш дискриминатор выиграл. В качестве окончательной ошибки попробуйте уменьшить скорость обучения дискриминатора. Это также может произойти, если заменить среднее значение по батчу на сумму или подобное.\n",
    " * Если он сходится к 0.5 и остается там в течение нескольких итераций, но генератор еще не научился генерировать правдоподобные данные, генератор проигрывает игру."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
