{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "В данном задании мы решим одну из самых простых задач представленных в gym 'CartPole-v0' с помощью метода policy gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n",
      "Starting virtual X frame buffer: Xvfb../xvfb: line 8: start-stop-daemon: command not found\n",
      ".\n",
      "env: DISPLAY=:1\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f935710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c504ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построим агента для  REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для алгоритма REINFORCE нам понадобится модель, которая предсказывает вероятности действий в заданных состояниях. Давайте определим такую модель ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построить простую нейронную сеть, которая предсказывает logits политики. \n",
    "#CartPole решается простой сетью.\n",
    "agent = nn.Sequential(\n",
    "    <Ваш код здесь.>\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # конвертируйте states, посчитайте logits, используйте softmax для получения вероятностного распределения\n",
    "    <ваш код здесь>\n",
    "    return < и здесь >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_proba(test_states)\n",
    "assert isinstance(test_probas, np.ndarray), \"верните nparray, а не %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (test_states.shape[0], n_actions), \"ошибочные размеры выхода: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis = 1), 1), \"вероятность не суммируется в 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    \n",
    "    #заведем массивы для наших состояний, действий и наград\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #распределение действий в текущем состоянии pi(a|s)\n",
    "        action_probas = predict_proba(np.array([s]))[0] \n",
    "\n",
    "        a = <семплируйте действие по распределению выше>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #созраняем состояния\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверим это\n",
    "states, actions, rewards = generate_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчет кумулятивной награды\n",
    "\n",
    "Имплементируйте функцию ниже:\n",
    "вам дается список награды за целую сессию\n",
    "посчитайте кумулятивную награду G(s,a) с понижением ее стоимости на каждом шаге.\n",
    "\n",
    "$G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...$\n",
    "\n",
    "Наиболее простой способ это сделать - итерироваться от последнего состояния к первому  и считать рекуррентно  $G_t = r_t + gamma*G_{t+1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #награды на каждом шаге\n",
    "                           gamma = 0.99 #discount для награды (коэффициент уменьшения ценности награды)\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Computation of cumulative rewards G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ..\n",
    "    :return: array of cummulative rewards\n",
    "    \"\"\"\n",
    "    <Вы должны вернуть массив / список накопительных вознаграждений с таким количеством элементов, как в первоначальных вознаграждениях.>\n",
    "    <Ваш код здесь>   \n",
    "    return <array of cumulative rewards>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"Успех!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Давайте теперь определим нашу целевую функцию и обновления с помощью policy gradient метода.\n",
    "\n",
    "Целевая функция:\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Следуя алгоритму REINFORCE, можно определить ее так\n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "Когда мы считаем градиент от нашей функции по параметрам $ \\theta $, то это становится в точности policy gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известная уже нам функция."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take an integer vector (tensor of variable) and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y.detach() if y.requires_grad else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    \n",
    "    return y_one_hot.requires_grad_() if y.requires_grad else y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим оптимизатор\n",
    "# Вы знаете что делать\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Приводим все к тензорам с градиентами\n",
    "    states = torch.tensor(states, requires_grad=True, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, requires_grad=True, dtype=torch.int)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, requires_grad=True, dtype=torch.float)\n",
    "    \n",
    "    # предсказывам logits, probas and log-probas используюя нашего агента. \n",
    "    logits = <your code here>\n",
    "    probas = <your code here>\n",
    "    logprobas = <your code here>\n",
    "    \n",
    "    assert all(v.requires_grad for v in [logits, probas, logprobas]), \\\n",
    "        \"не забывайте про градиенты у тензоров\"\n",
    "    \n",
    "    # выбираем log-probabilities для каждого действия, log pi(a_i|s_i)\n",
    "    logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions), dim = 1)\n",
    "    \n",
    "    # целевая функция для REINFORCE\n",
    "    J_hat = <policy objective as in the formula for J_hat. Please use mean, not sum.>\n",
    "    \n",
    "    #регуляризатор энтропией ( не обязательно ) \n",
    "    entropy_reg = <compute mean entropy of probas. Don't forget the sign!>\n",
    "    \n",
    "    loss = - J_hat - 0.1 * entropy_reg\n",
    "    \n",
    "    <Gradient descent step>\n",
    "    \n",
    "    # Возвращаем награды, чтобы потом из было удобно отрисовать\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    #генерируем новые сессии\n",
    "    rewards = [train_on_session(*generate_session()) for _ in range(100)] \n",
    "    \n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"Победа!\") \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.7822.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
